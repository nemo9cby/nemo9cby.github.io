This paper studies the source code changes of 1,288 revisions among 17 projects in Pharo ecosystem.
It contrasts source code changes against performance variations. 3 RQs have been raised and answered
with conclusions:
1. Most performance variations were caused by source code changes made in different methods.
2. 10 types of source code changes have been found, most of them are directly related to
method call addition, deletion or swap.
3. this paper implemented a new approach called horizontal profiling to reducing performance testing
overhead. By profiling only 17% of the versions, it is able to identify 83% of the performance 
regressions greater than 5% and 100% of the regressions greater than 50%.

pros:
1. I like the fact that they reached to authors to investigate the story. The results implied that
nearly 70% of the authors are not aware of the performance regression when they introduced changes.

2. The new approach is an upgrade method compared to Huang et al's work. It combined dynamic info
and static analysis results so that it does not need manual tuning. This sheds lights on other static
studies that we can leverage dynamic info when applying static code analysis.

3 It is good to take sample rate into consideration.

4. The finding of first RQ could be interesting to look deeper.

cons:
1. The results of the second  RQ is comprehensive but not earthshaking. It almost covers majority of
code changes and the categorization is not intuitive.

2. Section 4.1 they mentioned that they use a traditional code execution profiler without exposing its
name. THis could lead to difficulty of replication. Besides, it did not clarify that if there are any
impacts on the results if people choose different profiler to compare the results.

3. For some of the projets, the revision number is too small, which may lead to confusion in evaluation part
